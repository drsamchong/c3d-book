{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set mpl fonts\n",
    "plt.rcParams['font.sans-serif'] = ['Arial', 'Helvetica', 'Lucida Grande', 'Verdana']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for plotting\n",
    "def get_stats(y: pd.Series) -> tuple[float, float]:\n",
    "#    stats = (np.mean(y), np.std(y), np.corrcoef(x, y)[0][1])\n",
    "    stats = (y.mean(), y.std())\n",
    "    return stats\n",
    "\n",
    "def add_stats(ax, x, y):\n",
    "    mu, sigma = get_stats(y)\n",
    "    r = np.corrcoef(x, y)[0][1]\n",
    "    stats_text = (f'Mean y $\\\\mu$ = {mu:.2f}\\n'\n",
    "                  f'Stdev. y $\\\\sigma$ = {sigma:.2f}\\n'\n",
    "                  f'Correlation $r$ = {r:.2f}')\n",
    "    ax.text(0.95, 0.05, stats_text, fontsize=9, \n",
    "            transform=ax.transAxes, horizontalalignment='right')\n",
    "    \n",
    "def get_linear_fit(x, y):\n",
    "    model = np.polyfit(x, y, deg=1) # y = mx + c (m = gradient, c = intercept)\n",
    "    m, c = model\n",
    "    predict = np.poly1d(model)\n",
    "    r2 = r2_score(y, predict(x))\n",
    "    return m, c, r2\n",
    "\n",
    "def add_fit(ax, x, y):\n",
    "    m, c, r2 = get_linear_fit(x, y)\n",
    "    ax.axline(xy1=(0, c), slope=m, color=\"red\", linewidth=1) # xy1 provides defined point for line to pass through\n",
    "    fit_text = (f\"Fit\\nGradient: {m:.2f}\\nIntercept: {c:.2f}\\n\"\n",
    "                f\"y = {m:.2f}x + {c:.2f}\\n\"\n",
    "                f\"$r^2$ = {r2:.2f}\")\n",
    "    ax.text(0.05, 0.95, fit_text, fontsize=9, \n",
    "            transform=ax.transAxes, horizontalalignment='left', verticalalignment=\"top\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anscombe's quartet\n",
    "\n",
    "[Anscombe's quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet) (please don't click the link until after you have worked through the notebook) consists of four simple datasets, each consisting of 11 pairs of x and y values.\n",
    "\n",
    "Based on the statistical measures of mean, standard deviation and the correlation coefficient between the two variables, the datasets look close to identical. The data highlights the importance of visualisation in EDA and going beyond looking at the simple summary statistics.\n",
    "\n",
    "</br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The csv contains pairs of (xy) data, so (x1, y1) form the first set of data, (x2, y2) is the second, etc.\n",
    "\n",
    "anscombe_df = pd.read_csv(\"data/anscombe_quartet.csv\")\n",
    "anscombe_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the some simple summary statistics for the x and y columns of each dataset.\n",
    "\n",
    "The mean and standard deviation give a measure of the centre and its spread, so we can look at those. [Pearson's correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient), $r$ measures the linear correlation between the x and y variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cols = [\"y1\", \"y2\", \"y3\", \"y4\"]\n",
    "x_cols = [\"x1\", \"x2\", \"x3\", \"x4\"]\n",
    "\n",
    "print(\"Summary statistics for x data:\")\n",
    "display(anscombe_df[x_cols].describe().loc[[\"count\",\"mean\", \"std\"]])\n",
    "\n",
    "print(\"Summary statistics for y data:\")\n",
    "display(anscombe_df[y_cols].describe().loc[[\"count\",\"mean\", \"std\"]])\n",
    "\n",
    "\n",
    "for idx, dataset in enumerate(zip(x_cols, y_cols)):\n",
    "    print(f\"Dataset {idx+1}\")\n",
    "    pearson_r = np.corrcoef(anscombe_df[dataset[0]], anscombe_df[dataset[1]])[0][1]\n",
    "    print(f\"Pearson correlation coefficient: {pearson_r:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary statistics for each of the x columns and y columns are the same to two decimal places (the same precision as the data itself). The correlation coefficients are also the same.\n",
    "\n",
    "So from these measures, the four datasets look very similar. To check this, we can visualise the datasets as a set of scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting function\n",
    "def anscombe_plots(ans_df=anscombe_df, show_stats=False, show_fit=False):\n",
    "    \"\"\"Draw a 4x4 grid showing plots of the Anscombe quartet data\"\"\"\n",
    "    \n",
    "    INCLUDE_STATS = show_stats\n",
    "    INCLUDE_FIT = show_fit\n",
    "\n",
    "    fig, axs = plt.subplots(2,2, sharex=True, sharey=True, figsize=(8, 8))\n",
    "\n",
    "    for i, ax in enumerate(axs.flat):\n",
    "        x = ans_df.iloc[:, 2*i]\n",
    "        y = ans_df.iloc[:, 2*i+1]\n",
    "        ax.scatter(x, y)\n",
    "        ax.set_xlabel(\"x\")\n",
    "        ax.set_ylabel(\"y\")\n",
    "        data_label = f\"Data {i+1}\"\n",
    "        ax.text(0.95, 0.3, data_label, fontsize=16, transform=ax.transAxes, horizontalalignment=\"right\", verticalalignment='top')\n",
    "\n",
    "        if INCLUDE_STATS:\n",
    "            add_stats(ax, x, y)\n",
    "        \n",
    "        if INCLUDE_FIT:\n",
    "            add_fit(ax, x, y)\n",
    "\n",
    "\n",
    "    plt.xlim(0, 20)\n",
    "    plt.ylim(0, 14)\n",
    "    plt.xticks([5, 10, 15, 20])\n",
    "    plt.tight_layout()\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anscombe_plots(anscombe_df, show_stats=True, show_fit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the scatter plots we can immediately see that the datasets are quite distinct. \n",
    "If we calculate a linear regression line through the data, do the coefficients of the equation help to distinguish the data numerically?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anscombe_plots(anscombe_df, show_stats=True, show_fit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best fit line is also identical and the r-squared value indicates that numerically, at least, it fits all of the datasets equally well (or badly).\n",
    "\n",
    "However, we can see from the visualisations that the regression line is only really appropriate for the first dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at some of the other descriptive statistics, particularly relating to the data distribution (max/min, percentiles), these show there are differences between the data. But the simple scatter plot visualisations make the contrasts unmissable.\n",
    "\n",
    "In the case of Anscombe's quartet, the scatter plots are sufficient to highlight the differences. For more complex data, there are other visualisations that highlight the differences between the distributions of the datasets. The [seaborn](https://seaborn.pydata.org/index.html) visualisation library makes it particularly straightforward to look at distributions of multiple variables and correlations between variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Key points summary**\n",
    "\n",
    "- Visualisation is an essential part of exploratory data analysis.\n",
    "  - Always explore your datasets visually before applying models or drawing conclusions.\n",
    "- “The first step in analyzing data is to look at it.” – John Tukey\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Things to try/consider\n",
    "\n",
    "In the case of Anscombe's quartet, the scatter plots are sufficient to highlight the differences. \n",
    "\n",
    "For more complex data, there are other visualisations that highlight the differences between the distributions of the datasets. The [seaborn](https://seaborn.pydata.org/index.html) visualisation library makes it particularly straightforward to look at distributions of multiple variables and correlations between variables.\n",
    "\n",
    "What kinds of plot might help to highlight the differences between the datasets or identify issues that might affect how the data is modelled, e.g. for dataset 3, there is a single isolated point; how does this affect the best-fit linear model; how might this be identified?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chem502",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
